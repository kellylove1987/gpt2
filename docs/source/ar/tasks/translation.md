# ุงูุชุฑุฌูุฉ

[[open-in-colab]]

<Youtube id="1JvfrvZgi6c"/>

ุชุนุฏ ุงูุชุฑุฌูุฉ ุฅุญุฏู ุงูููุงู ุงูุชู ููููู ุตูุงุบุชูุง ููุดููุฉ ุชุณูุณู ุฅูู ุชุณูุณูุ ููู ุฅุทุงุฑ ุนูู ููู ูุฅุฑุฌุงุน ุจุนุถ ุงููุฎุฑุฌุงุช ูู ุงูุฅุฏุฎุงูุ ูุซู ุงูุชุฑุฌูุฉ ุฃู ุงูููุฎุต. ูุชูุณุชุฎุฏู ุฃูุธูุฉ ุงูุชุฑุฌูุฉ ุนุงุฏุฉ ูุชุฑุฌูุฉ ุงููุตูุต ุจูู ุงููุบุงุช ุงููุฎุชููุฉุ ูููู ูููู ุฃูุถูุง ุงุณุชุฎุฏุงููุง ููููุงู ุฃู ุจุนุถ ุงููุฒุฌ ุจููููุง ูุซู ุชุญููู ุงููุต ุฅูู ููุงู ุฃู ุงูููุงู ุฅูู ูุต.

ุณููุถุญ ูุฐุง ุงูุฏููู ููููุฉ:

1. ุถุจุท ูููุฐุฌ T5 ุงูุฏููู ุนูู ุงูุฌุฒุก ุงููุฑุนู ุงูุฅูุฌููุฒู ุงููุฑูุณู ูู ูุฌููุนุฉ ุจูุงูุงุช OPUS Books ูุชุฑุฌูุฉ ุงููุต ุงูุฅูุฌููุฒู ุฅูู ุงููุฑูุณูุฉ.
2. ุงุณุชุฎุฏุงู ูููุฐุฌู ุงูุฏููู ููุงุณุชูุชุงุฌ.

<Tip>

ูุนุฑุถ ุฌููุน ุงูุจููุงุช ูููุงุท ุงููุฑุงูุจุฉ ุงููุชูุงููุฉ ูุน ูุฐู ุงููููุฉุ ููุตู ุจุงูุชุญูู ูู [ุตูุญุฉ ุงููููุฉ](https://huggingface.co/tasks/translation).

</Tip>

ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ:

```bash
pip install transformers datasets evaluate sacrebleu
```

ูุดุฌุนู ุนูู ุชุณุฌูู ุงูุฏุฎูู ุฅูู ุญุณุงุจ Hugging Face ุงูุฎุงุต ุจู ุญุชู ุชุชููู ูู ุชุญููู ูููุฐุฌู ููุดุงุฑูุชู ูุน ุงููุฌุชูุน. ุนูุฏูุง ููุทูุจ ููู ุฐููุ ุฃุฏุฎู ุฑูุฒู ููุชุณุฌูู:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

## ุชุญููู ูุฌููุนุฉ ุจูุงูุงุช OPUS Books

ุงุจุฏุฃ ุจุชุญููู ุงูุฌุฒุก ุงููุฑุนู ุงูุฅูุฌููุฒู ุงููุฑูุณู ูู ูุฌููุนุฉ ุจูุงูุงุช [OPUS Books](https://huggingface.co/datasets/opus_books) ูู ููุชุจุฉ Datasets ๐ค:

```py
>>> from datasets import load_dataset

>>> books = load_dataset("opus_books", "en-fr")
```

ูุณููู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู ูุฌููุนุงุช ุชุฏุฑูุจ ูุงุฎุชุจุงุฑ ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [`~datasets.Dataset.train_test_split`]:

```py
>>> books = books["train"].train_test_split(test_size=0.2)
```

ุซู ุงูู ูุธุฑุฉ ุนูู ูุซุงู:

```py
>>> books["train"][0]
{'id': '90560',
 'translation': {'en': 'But this lofty plateau measured only a few fathoms, and soon we reentered Our Element.',
  'fr': 'Mais ce plateau รฉlevรฉ ne mesurait que quelques toises, et bientรดt nous fรปmes rentrรฉs dans notre รฉlรฉment.'}}
```

`translation`: ุชุฑุฌูุฉ ุฅูุฌููุฒูุฉ ููุฑูุณูุฉ ูููุต.

## ูุนุงูุฌุฉ ูุณุจูุฉ

<Youtube id="XAR8jnZZuUs"/>

ุงูุฎุทูุฉ ุงูุชุงููุฉ ูู ุชุญููู ุจุฑูุงูุฌ ุชุดููุฑ T5 ููุนุงูุฌุฉ ุฃุฒูุงุฌ ุงููุบุงุช ุงูุฅูุฌููุฒูุฉ ูุงููุฑูุณูุฉ:

```py
>>> from transformers import AutoTokenizer

>>> checkpoint = "google-t5/t5-small"
>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)
```

ูุฌุจ ุฃู ุชููู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุงูุชู ุชุฑูุฏ ุฅูุดุงุฆูุง ุจูุง ููู:

1. ุฅุถุงูุฉ ุจุงุฏุฆุฉ ุฅูู ุงูุฅุฏุฎุงู ุจุงุณุชุฎุฏุงู ููุฌู ุญุชู ูุนุฑู T5 ุฃู ูุฐู ูููุฉ ุชุฑุฌูุฉ. ุชุชุทูุจ ุจุนุถ ุงูููุงุฐุฌ ุงููุงุฏุฑุฉ ุนูู ููุงู NLP ูุชุนุฏุฏุฉ ุงููุทุงูุจุฉ ุจููุงู ูุญุฏุฏุฉ.
2. ูู ุจุชุดููุฑ ุงูุฅุฏุฎุงู (ุงูุฅูุฌููุฒูุฉ) ูุงููุฏู (ุงููุฑูุณูุฉ) ุจุดูู ูููุตู ูุฃูู ูุง ููููู ุชุดููุฑ ุงููุต ุงููุฑูุณู ุจุงุณุชุฎุฏุงู ุจุฑูุงูุฌ ุชุดููุฑ ุชู ุชุฏุฑูุจู ูุณุจููุง ุนูู ููุฑุฏุงุช ุงููุบุฉ ุงูุฅูุฌููุฒูุฉ.
3. ุงูุทุน ุงูุชุณูุณูุงุช ุจุญูุซ ูุง ุชููู ุฃุทูู ูู ุงูุทูู ุงูุฃูุตู ุงููุญุฏุฏ ุจูุงุณุทุฉ ูุนููุฉ "max_length".

```py
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "
```py
>>> source_lang = "en"
>>> target_lang = "fr"
>>> prefix = "translate English to French: "


>>> def preprocess_function(examples):
...     inputs = [prefix + example[source_lang] for example in examples["translation"]]
...     targets = [example[target_lang] for example in examples["translation"]]
...     model_inputs = tokenizer(inputs, text_target=targets, max_length=128, truncation=True)
...     return model_inputs
```

ูุชุทุจูู ุฏุงูุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุงุ ุงุณุชุฎุฏู ุทุฑููุฉ [`~datasets.Dataset.map`] ูู ููุชุจุฉ Datasets ๐ค. ููููู ุชุณุฑูุน ูุธููุฉ "map" ุนู ุทุฑูู ุชุนููู "batched=True" ููุนุงูุฌุฉ ุนูุงุตุฑ ูุชุนุฏุฏุฉ ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูู ููุช ูุงุญุฏ:

```py
>>> tokenized_books = books.map(preprocess_function, batched=True)
```

ุงูุขู ูู ุจุฅูุดุงุก ุฏูุนุฉ ูู ุงูุฃูุซูุฉ ุจุงุณุชุฎุฏุงู [`DataCollatorForSeq2Seq`]. ูู ุงูุฃูุซุฑ ููุงุกุฉ *ุชุนุจุฆุฉ* ุงูุฌูู ุฏููุงูููููุง ุฅูู ุงูุทูู ุงูุฃุทูู ูู ุฏูุนุฉ ุฃุซูุงุก ุงูุชุฌููุนุ ุจุฏูุงู ูู ุชุนุจุฆุฉ ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุฅูู ุงูุทูู ุงูุฃูุตู.

<frameworkcontent>
<pt>
  
```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)
```
</pt>
<tf>

```py
>>> from transformers import DataCollatorForSeq2Seq

>>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors="tf")
```
</tf>
</frameworkcontent>

## ุชูููู

ุบุงูุจูุง ูุง ูููู ุชุถููู ูููุงุณ ุฃุซูุงุก ุงูุชุฏุฑูุจ ูููุฏูุง ูุชูููู ุฃุฏุงุก ูููุฐุฌู. ููููู ุชุญููู ุทุฑููุฉ ุชูููู ุจุณุฑุนุฉ ุจุงุณุชุฎุฏุงู ููุชุจุฉ ๐ค [Evaluate](https://huggingface.co/docs/evaluate/index). ุจุงููุณุจุฉ ููุฐู ุงููููุฉุ ูู ุจุชุญููู ูููุงุณ [SacreBLEU](https://huggingface.co/spaces/evaluate-metric/sacrebleu) (ุฑุงุฌุน ุฏููู ุงููุณุชุฎุฏู ุงูุฎุงุต ุจู ๐ค Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour) ููุนุฑูุฉ ุงููุฒูุฏ ุญูู ููููุฉ ุชุญููู ูุญุณุงุจ ูููุงุณ):

```py
>>> import evaluate

>>> metric = evaluate.load("sacrebleu")
```

ุซู ูู ุจุฅูุดุงุก ุฏุงูุฉ ุชูุฑุฑ ุชูุจุคุงุชู ูุชุตูููุงุชู ุฅูู [`~evaluate.EvaluationModule.compute`] ูุญุณุงุจ ุฏุฑุฌุฉ SacreBLEU:

```py
>>> import numpy as np


>>> def postprocess_text(predsุ labels):
...     preds = [pred.strip() for pred in preds]
...     labels = [[label.strip()] for label in labels]

...     return preds, labels


>>> def compute_metrics(eval_preds):
...     preds, labels = eval_preds
...     if isinstance(preds, tuple):
...         preds = preds[0]
...     decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)

...     labels = np.where(labels != -100, labels, tokenizer.pad_token_id)
...     decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)

...     decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)

...     result = metric.compute(predictions=decoded_preds, references=decoded_labels)
...     result = {"bleu": result["score"]}

...     prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]
...     result["gen_len"] = np.mean(prediction_lens)
...     result = {k: round(v, 4) for k, v in result.items()}
...     return result
```

ุฏุงูุชู `compute_metrics` ุฌุงูุฒุฉ ุงูุขูุ ูุณุชุนูุฏ ุฅูููุง ุนูุฏ ุฅุนุฏุงุฏ ุงูุชุฏุฑูุจ.

## ุชุฏุฑูุจ

<frameworkcontent>
<pt>
<Tip>

ุฅุฐุง ูู ุชูู ุนูู ุฏุฑุงูุฉ ุจุถุจุท ูููุฐุฌ ุจุงุณุชุฎุฏุงู [`Trainer`ุ ูุฑุงุฌุน ุงูุจุฑูุงูุฌ ุงูุชุนูููู ุงูุฃุณุงุณู [here](../training#train-with-pytorch-trainer)!

</Tip>

ุฃูุช ูุณุชุนุฏ ุงูุขู ูุจุฏุก ุชุฏุฑูุจ ูููุฐุฌู! ูู ุจุชุญููู T5 ุจุงุณุชุฎุฏุงู [`AutoModelForSeq2SeqLM`]:

```py
>>> from transformers import AutoModelForSeq2SeqLM, Seq2SeqTrainingArguments, Seq2SeqTrainer

>>> model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

ูู ูุฐู ุงููุฑุญูุฉุ ููุงู ุซูุงุซ ุฎุทูุงุช ููุท:

1. ุญุฏุฏ ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ ุงูุฎุงุตุฉ ุจู ูู [`Seq2SeqTrainingArguments`]. ุงููุนููุฉ ุงููุทููุจุฉ ุงููุญูุฏุฉ ูู `output_dir` ุงูุชู ุชุญุฏุฏ ููุงู ุญูุธ ูููุฐุฌู. ุณุชููู ุจุงูุฏูุน ุฅูู Hub ุนู ุทุฑูู ุชุนููู `push_to_hub=True` (ูุฌุจ ุชุณุฌูู ุงูุฏุฎูู ุฅูู Hugging Face ูุชุญููู ูููุฐุฌู). ูู ููุงูุฉ ูู ุญูุจุฉุ ุณูููู [`Trainer`] ุจุชูููู ูููุงุณ SacreBLEU ูุญูุธ ููุทุฉ ุงููุฑุงูุจุฉ ุงูุชุฏุฑูุจูุฉ.
2. ูุฑุฑ ุญุฌุฉ ุงูุชุฏุฑูุจ ุฅูู [`Seq2SeqTrainer`] ุฌูุจูุง ุฅูู ุฌูุจ ูุน ุงููููุฐุฌ ููุฌููุนุฉ ุงูุจูุงูุงุช ูุจุฑูุงูุฌ ุงูุชุฑููุฒ ููุฌูููุน ุงูุจูุงูุงุช ููุธููุฉ `compute_metrics`.
3. ุงุณุชุฏุนุงุก [`~Trainer.train`] ูุถุจุท ูููุฐุฌู ุจุดูู ุฏููู.

```py
>>> training_args = Seq2SeqTrainingArguments(
...     output_dir="my_awesome_opus_books_model"ุ
...     eval_strategy="epoch"ุ
...     learning_rate=2e-5ุ
...     per_device_train_batch_size=16ุ
...     per_device_eval_batch_size=16ุ
...     weight_decay=0.01ุ
...     save_total_limit=3ุ
...     num_train_epochs=2ุ
...     predict_with_generate=Trueุ
...     fp16=Trueุ
...     push_to_hub=Trueุ
... )

>>> trainer = Seq2SeqTrainer(
...     model=modelุ
...     args=training_argsุ
...     train_dataset=tokenized_books["train"]ุ
...     eval_dataset=tokenized_books["test"]ุ
...     tokenizer=tokenizerุ
...     data_collator=data_collatorุ
...     compute_metrics=compute_metricsุ
... )

>>> trainer.train()
```

ุจูุฌุฑุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ุดุงุฑู ูููุฐุฌู ุนูู Hub ุจุงุณุชุฎุฏุงู ุทุฑููุฉ [`~transformers.Trainer.push_to_hub`] ุญุชู ูุชููู ุงูุฌููุน ูู ุงุณุชุฎุฏุงู ูููุฐุฌู:

```py
>>> trainer.push_to_hub()
```
</pt>
<tf>
<Tip>

ุฅุฐุง ูู ุชูู ุนูู ุฏุฑุงูุฉ ุจุถุจุท ูููุฐุฌ ุจุงุณุชุฎุฏุงู Kerasุ ูุฑุงุฌุน ุงูุจุฑูุงูุฌ ุงูุชุนูููู ุงูุฃุณุงุณู [here](../training#train-a-tensorflow-model-with-keras)!

</Tip>
ูุถุจุท ูููุฐุฌ ุฏููู ูู TensorFlowุ ุงุจุฏุฃ ุจุฅุนุฏุงุฏ ุฏุงูุฉ ูุญุณู ููุนุฏู ุชุนูู ูุจุนุถ ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ:

```py
>>> from transformers import AdamWeightDecay

>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)
```

ุจุนุฏ ุฐููุ ููููู ุชุญููู T5 ุจุงุณุชุฎุฏุงู [`TFAutoModelForSeq2SeqLM`]:

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)
```

ูู ุจุชุญููู ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุฅูู ุชูุณูู `tf.data.Dataset` ุจุงุณุชุฎุฏุงู [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:

```py
>>> tf_train_set = model.prepare_tf_dataset(
...     tokenized_books["train"]ุ
...     shuffle=Trueุ
...     batch_size=16ุ
...     collate_fn=data_collatorุ
... )

>>> tf_test_set = model.prepare_tf_dataset(
...     tokenized_books["test"]ุ
...     shuffle=Falseุ
...     batch_size=16ุ
...     collate_fn=data_collatorุ
... )
```

ูู ุจุชูููู ุงููููุฐุฌ ููุชุฏุฑูุจ ุจุงุณุชุฎุฏุงู [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). ูุงุญุธ ุฃู ุฌููุน ููุงุฐุฌ Transformers ุจูุง ุฏุงูุฉ ุฎุณุงุฑุฉ ุงูุชุฑุงุถูุฉ ุฐุงุช ุตูุฉ ุจุงููููุฉุ ูุฐูู ูุง ุชุญุชุงุฌ ุฅูู ุชุญุฏูุฏ ูุงุญุฏุฉ ูุง ูู ุชุฑุบุจ ูู ุฐูู:

```py
>>> import tensorflow as tf

>>> model.compile(optimizer=optimizer) # ูุง ุชูุฌุฏ ุญุฌุฉ ุงูุฎุณุงุฑุฉ!
```

ุงูุฃูุฑุงู ุงูุฃุฎูุฑุงู ุงููุฐุงู ูุฌุจ ุฅุนุฏุงุฏููุง ูุจู ุจุฏุก ุงูุชุฏุฑูุจ ููุง ุญุณุงุจ ูููุงุณ SacreBLEU ูู ุงูุชูุจุคุงุชุ ูุชูููุฑ ุทุฑููุฉ ูุฏูุน ูููุฐุฌู ุฅูู Hub. ูุชู ุฐูู ุจุงุณุชุฎุฏุงู [Keras callbacks](../main_classes/keras_callbacks).

ูุฑุฑ ุฏุงูุชู `compute_metrics` ุฅูู [`~transformers.KerasMetricCallback`]:

```py
>>> from transformers.keras_callbacks import KerasMetricCallback

>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)
```

ุญุฏุฏ ุงูููุงู ุงูุฐู ุณุชุฏูุน ููู ูููุฐุฌู ูุจุฑูุงูุฌ ุงูุชุฑููุฒ ุงูุฎุงุต ุจู ูู [`~transformers.PushToHubCallback`]:

```py
>>> from transformers.keras_callbacks import PushToHubCallback

>>> push_to_hub_callback = PushToHubCallback(
...     output_dir="my_awesome_opus_books_model"ุ
...     tokenizer=tokenizerุ
... )
```

ุจุนุฏ ุฐููุ ูู ุจุชุฌููุน ููุงููุงุชู ูุฑุฉ ุฃุฎุฑู:

```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```
ุซู ูู ุจุชุฌููุน ููุงููุงุชู ูุฑุฉ ุฃุฎุฑู:

```py
>>> callbacks = [metric_callback, push_to_hub_callback]
```

ุฃุฎูุฑูุงุ ุฃูุช ูุณุชุนุฏ ูุจุฏุก ุชุฏุฑูุจ ูููุฐุฌู! ุงุณุชุฏุนุงุก [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ูุน ูุฌููุนุงุช ุงูุจูุงูุงุช ุงูุชุฏุฑูุจูุฉ ูุงูุชุญูู ูู ุตุญุชูุงุ ูุนุฏุฏ ุงูุนุตูุฑุ ูููุงููุงุชู ูุถุจุท ูููุฐุฌู:

```py
>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)
```

ุจูุฌุฑุฏ ุงูุชูุงู ุงูุชุฏุฑูุจุ ูุชู ุชุญููู ูููุฐุฌู ุชููุงุฆููุง ุฅูู Hub ุญุชู ูุชููู ุงูุฌููุน ูู ุงุณุชุฎุฏุงูู!
</tf>
</frameworkcontent>

<Tip>

ููุญุตูู ุนูู ูุซุงู ุฃูุซุฑ ุดูููุงู ุญูู ููููุฉ ุถุจุท ูููุฐุฌ ููุชุฑุฌูุฉุ ุฑุงุฌุน ุงูุฏูุชุฑ ุงูููุงุณุจ
[ุฏูุชุฑ ููุงุญุธุงุช PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation.ipynb)
ุฃู [ุฏูุชุฑ ููุงุญุธุงุช TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/translation-tf.ipynb).

</Tip>

## ุงูุงุณุชูุชุงุฌ

ุฑุงุฆุนุ ุงูุขู ุจุนุฏ ุฃู ุถุจุทุช ูููุฐุฌูุงุ ููููู ุงุณุชุฎุฏุงูู ููุงุณุชูุชุงุฌ!

ููุฑ ูู ุจุนุถ ุงููุตูุต ุงูุชู ุชุฑุบุจ ูู ุชุฑุฌูุชูุง ุฅูู ูุบุฉ ุฃุฎุฑู. ุจุงููุณุจุฉ ูู T5ุ ูุฌุจ ุฅุถุงูุฉ ุจุงุฏุฆุฉ ุฅูู ุฅุฏุฎุงูู ููููุง ูููููุฉ ุงูุชู ุชุนูู ุนูููุง. ููุชุฑุฌูุฉ ูู ุงูุฅูุฌููุฒูุฉ ุฅูู ุงููุฑูุณูุฉุ ูุฌุจ ุฅุถุงูุฉ ุจุงุฏุฆุฉ ุฅูู ุฅุฏุฎุงูู ููุง ูู ููุถุญ ุฃุฏูุงู:

```py
>>> text = "translate English to French: Legumes share resources with nitrogen-fixing bacteria."
```

ุฃุจุณุท ุทุฑููุฉ ูุชุฌุฑุจุฉ ูููุฐุฌู ุงูุฏููู ููุงุณุชูุชุงุฌ ูู ุงุณุชุฎุฏุงูู ูู [`pipeline`]. ูู ุจุชูููุฐ ูุซูู ูู `pipeline` ููุชุฑุฌูุฉ ุจุงุณุชุฎุฏุงู ูููุฐุฌูุ ููุฑุฑ ูุตู ุฅููู:

```py
>>> from transformers import pipeline

# ุชุบููุฑ `xx` ุฅูู ูุบุฉ ุงูุฅุฏุฎุงู ู`yy` ุฅูู ูุบุฉ ุงูุฅุฎุฑุงุฌ ุงููุทููุจุฉ.
# ุฃูุซูุฉ: "en" ููุฅูุฌููุฒูุฉุ "fr" ูููุฑูุณูุฉุ "de" ููุฃููุงููุฉุ "es" ููุฅุณุจุงููุฉุ "zh" ููุตูููุฉุ ุฅูุฎุ translation_en_to_fr ูุชุฑุฌู ูู ุงูุฅูุฌููุฒูุฉ ุฅูู ุงููุฑูุณูุฉ
# ููููู ุนุฑุถ ุฌููุน ููุงุฆู ุงููุบุงุช ููุง - https://huggingface.co/languages
>>> translator = pipeline("translation_xx_to_yy"ุ model="my_awesome_opus_books_model")
>>> translator(text)
[{'translation_text': 'Legumes partagent des ressources avec des bactรฉries azotantes.'}]
```

ููููู ุฃูุถูุง ุฅุนุงุฏุฉ ุฅูุชุงุฌ ูุชุงุฆุฌ `pipeline` ูุฏูููุง:

<frameworkcontent>
<pt>
ูู ุจุชุฑููุฒ ุงููุต ูุฅุฑุฌุงุน `input_ids` ูุฑููุฒ PyTorch:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(text, return_tensors="pt").input_ids
```

ุงุณุชุฎุฏู ุทุฑููุฉ [`~generation.GenerationMixin.generate`] ูุฅูุดุงุก ุงูุชุฑุฌูุฉ. ููุฒูุฏ ูู ุงูุชูุงุตูู ุญูู ุงุณุชุฑุงุชูุฌูุงุช ุชูููุฏ ุงููุต ุงููุฎุชููุฉ ูุงููุนููุงุช ููุชุญูู ูู ุงูุชูููุฏุ ุฑุงุฌุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช [Text Generation](../main_classes/text_generation).

```py
>>> from transformers import AutoModelForSeq2SeqLM

>>> model = AutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)
```
ูู ุจูู ุชุดููุฑ ุฑููุฒ ุงููุนุฑูุงุช ุงููููุฏุฉ ูุฑุฉ ุฃุฎุฑู ุฅูู ูุต:

```py
>>> tokenizer.decode(outputs[0]ุ skip_special_tokens=True)
"Les lignรฉes partagent des ressources avec des bactรฉries enfixant l'azote."
```
</pt>
<tf>
ูู ุจุฑูุฒ ุงููุต ูุฅุฑุฌุงุน `input_ids` ูุฑููุฒ TensorFlow:

```py
>>> from transformers import AutoTokenizer

>>> tokenizer = AutoTokenizer.from_pretrained("my_awesome_opus_books_model")
>>> inputs = tokenizer(textุ return_tensors="tf").input_ids
```

ุงุณุชุฎุฏู ุทุฑููุฉ [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] ูุฅูุดุงุก ุงูุชุฑุฌูุฉ. ููุฒูุฏ ูู ุงูุชูุงุตูู ุญูู ุงุณุชุฑุงุชูุฌูุงุช ุฅูุดุงุก ุงููุตูุต ุงููุฎุชููุฉ ููุนููุงุช ุงูุชุญูู ูู ุงูุฅูุดุงุกุ ุฑุงุฌุน ูุงุฌูุฉ ุจุฑูุฌุฉ ุงูุชุทุจููุงุช [Text Generation](../main_classes/text_generation).

```py
>>> from transformers import TFAutoModelForSeq2SeqLM

>>> model = TFAutoModelForSeq2SeqLM.from_pretrained("my_awesome_opus_books_model")
>>> outputs = model.generate(inputsุ max_new_tokens=40ุ do_sample=Trueุ top_k=30ุ top_p=0.95)
```

ูู ุจูู ุชุดููุฑ ุฑููุฒ ุงููุนุฑูุงุช ุงููููุฏุฉ ูุฑุฉ ุฃุฎุฑู ุฅูู ูุต:

```py
>>> tokenizer.decode(outputs[0]ุ skip_special_tokens=True)
"Les lugumes partagent les ressources avec des bactรฉries fixatrices d'azote."
```
</tf>
</frameworkcontent>