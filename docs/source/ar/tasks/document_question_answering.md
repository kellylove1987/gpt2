# ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุงุฑุฏุฉ ูู ุงููุซููุฉ

[[open-in-colab]]

ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุงุฑุฏุฉ ูู ุงููุซููุฉุ ูุงูุชู ูุดุงุฑ ุฅูููุง ุฃูุถูุง ุจุงุณู ุงูุฅุฌุงุจุฉ ุงููุฑุฆูุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุงุฑุฏุฉ ูู ุงููุซููุฉุ ูู ูููุฉ ุชุชุถูู ุชูุฏูู ุฅุฌุงุจุงุช ุนูู ุงูุฃุณุฆูุฉ ุงููุทุฑูุญุฉ ุญูู ุตูุฑ ุงููุณุชูุฏุงุช. ุงููุฏุฎูุงุช ุฅูู ุงูููุงุฐุฌ ุงูุชู ุชุฏุนู ูุฐู ุงููููุฉ ูู ุนุงุฏุฉ ูุฒูุฌ ูู ุงูุตูุฑุฉ ูุงูุณุคุงูุ ูุงููุงุชุฌ ูู ุฅุฌุงุจุฉ ูุนุจุฑ ุนููุง ุจุงููุบุฉ ุงูุทุจูุนูุฉ. ุชุณุชุฎุฏู ูุฐู ุงูููุงุฐุฌ ุฃูุถุงุนูุง ูุชุนุฏุฏุฉุ ุจูุง ูู ุฐูู ุงููุตุ ูููุงุถุน ุงููููุงุช (ุญุฏูุฏ ุงูุฅุญุฏุงุซูุงุช)ุ ูุงูุตูุฑุฉ ููุณูุง.

ููุถุญ ูุฐุง ุงูุฏููู ููููุฉ:

- ุถุจุท ูููุฐุฌ LayoutLMv2 ุงูุฏููู ุนูู ูุฌููุนุฉ ุจูุงูุงุช DocVQA.
- ุงุณุชุฎุฏุงู ูููุฐุฌู ุงููุถุจูุท ุฏููููุง ููุงุณุชูุชุงุฌ.

<Tip>

ููุนุฑูุฉ ุฌููุน ุงูุชุตูููุงุช ูููุงุท ุงูุชุญูู ุงููุชูุงููุฉ ูุน ูุฐู ุงููููุฉุ ููุตู ุจุงูุชุญูู ูู [ุตูุญุฉ ุงููููุฉ](https://huggingface.co/tasks/image-to-text)

</Tip>

ูุญู LayoutLMv2 ูููุฉ ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุงุฑุฏุฉ ูู ุงููุซููุฉ ุนู ุทุฑูู ุฅุถุงูุฉ ุฑุฃุณ ููุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุฃุนูู ุญุงูุงุช ุงูุฑููุฒ ุงูููุงุฆูุฉ ููุฑููุฒุ ููุชูุจุค ุจููุงุถุน ุฑููุฒ ุงูุจุฏุก ูุงูููุงูุฉ ููุฅุฌุงุจุฉ. ูุจุนุจุงุฑุฉ ุฃุฎุฑูุ ุชุชู ูุนุงููุฉ ุงููุดููุฉ ุนูู ุฃููุง ุฅุฌุงุจุฉ ุงุณุชุฎุฑุงุฌูุฉ: ุงุณุชุฎุฑุงุฌ ูุทุนุฉ ุงููุนูููุงุช ุงูุชู ุชุฌูุจ ุนูู ุงูุณุคุงูุ ุจุงููุธุฑ ุฅูู ุงูุณูุงู. ูุฃุชู ุงูุณูุงู ูู ุฅุฎุฑุงุฌ ูุญุฑู ุงูุชุนุฑู ุงูุถูุฆู ุนูู ุงูุญุฑููุ ููู ููุง Tesseract ูู Google.

ูุจู ุงูุจุฏุกุ ุชุฃูุฏ ูู ุชุซุจูุช ุฌููุน ุงูููุชุจุงุช ุงูุถุฑูุฑูุฉ. ูุนุชูุฏ LayoutLMv2 ุนูู detectron2 ู torchvision ู tesseract.

```bash
pip install -q transformers datasets
```

```bash
pip install 'git+https://github.com/facebookresearch/detectron2.git'
pip install torchvision
```

```bash
sudo apt install tesseract-ocr
pip install -q pytesseract
```

ุจูุฌุฑุฏ ุชุซุจูุช ุฌููุน ุงูุชุจุนูุงุชุ ุฃุนุฏ ุชุดุบูู ููุช ุงูุชุดุบูู ุงูุฎุงุต ุจู.

ูุญู ูุดุฌุนู ุนูู ูุดุงุฑูุฉ ูููุฐุฌู ูุน ุงููุฌุชูุน. ูู ุจุชุณุฌูู ุงูุฏุฎูู ุฅูู ุญุณุงุจ Hugging Face ุงูุฎุงุต ุจู ูุชุญูููู ุฅูู ๐ค Hub.
ุนูุฏ ุงููุทุงูุจุฉุ ุฃุฏุฎู ุฑูุฒู ููุชุณุฌูู:

```py
>>> from huggingface_hub import notebook_login

>>> notebook_login()
```

ุฏุนููุง ูุญุฏุฏ ุจุนุถ ุงููุชุบูุฑุงุช ุงูุนุงูููุฉ.

```py
>>> model_checkpoint = "microsoft/layoutlmv2-base-uncased"
>>> batch_size = 4
```

## ุชุญููู ุงูุจูุงูุงุช

ูู ูุฐุง ุงูุฏูููุ ูุณุชุฎุฏู ุนููุฉ ุตุบูุฑุฉ ูู DocVQA ุงููุนุงูุฌุฉ ูุณุจููุง ูุงูุชู ููููู ุงูุนุซูุฑ ุนูููุง ุนูู ๐ค Hub. ุฅุฐุง ููุช ุชุฑุบุจ ูู ุงุณุชุฎุฏุงู ูุฌููุนุฉ DocVQA ุงููุงููุฉุ ูููููู ุงูุชุณุฌูู ูุชูุฒูููุง ูู [ุงูุตูุญุฉ ุงูุฑุฆูุณูุฉ ูู DocVQA](https://rrc.cvc.uab.es/?ch=17). ุฅุฐุง ููุช ุจุฐููุ ูููุชุงุจุนุฉ ูุน ูุฐุง ุงูุฏูููุ ุชุญูู ูู [ููููุฉ ุชุญููู ุงููููุงุช ุฅูู ูุฌููุนุฉ ุจูุงูุงุช ๐ค](https://huggingface.co/docs/datasets/loading#local-and-remote-files).

```py
>>> from datasets import load_dataset

>>> dataset = load_dataset("nielsr/docvqa_1200_examples")
>>> dataset
DatasetDict({
    train: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 1000
    })
    test: Dataset({
        features: ['id', 'image', 'query', 'answers', 'words', 'bounding_boxes', 'answer'],
        num_rows: 200
    })
})
```

ููุง ุชุฑูุ ุชู ุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู ูุฌููุนุงุช ุชุฏุฑูุจ ูุงุฎุชุจุงุฑ ุจุงููุนู. ุงูู ูุธุฑุฉ ุนูู ูุซุงู ุนุดูุงุฆู ููุชุนุฑู ุนูู ุงูููุฒุงุช.

```py
>>> dataset["train"].features
```
ููุง ุชุฑูุ ุชู ุชูุณูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุฅูู ูุฌููุนุงุช ุชุฏุฑูุจ ูุงุฎุชุจุงุฑ ุจุงููุนู. ุงูู ูุธุฑุฉ ุนูู ูุซุงู ุนุดูุงุฆู ููุชุนุฑู ุนูู ุงูููุฒุงุช.

```py
>>> dataset["train"].features
```

ูุฐุง ูุง ุชูุซูู ุงูุญููู ุงููุฑุฏูุฉ:
* `id`: ูุนุฑู ุงููุซุงู
* `image`: ูุงุฆู PIL.Image.Image ูุญุชูู ุนูู ุตูุฑุฉ ุงููุณุชูุฏ
* `query`: ุณูุณูุฉ ุงูุงุณุชุนูุงู - ุณุคุงู ุงููุบุฉ ุงูุทุจูุนูุฉ ุงููุทุฑูุญุ ุจุนุฏุฉ ูุบุงุช
* `answers`: ูุงุฆูุฉ ุงูุฅุฌุงุจุงุช ุงูุตุญูุญุฉ ุงูุชู ูุฏููุง ุงููุนูููู ุงูุจุดุฑููู
* `words` ู `bounding_boxes`: ูุชุงุฆุฌ ุงูุชุนุฑู ุงูุถูุฆู ุนูู ุงูุญุฑููุ ูุงูุชู ูู ูุณุชุฎุฏููุง ููุง
* `answer`: ุฅุฌุงุจุฉ ุชูุช ูุทุงุจูุชูุง ุจูุงุณุทุฉ ูููุฐุฌ ูุฎุชูู ูู ูุณุชุฎุฏูู ููุง

ุฏุนููุง ูุชุฑู ููุท ุงูุฃุณุฆูุฉ ุจุงููุบุฉ ุงูุฅูุฌููุฒูุฉุ ููููู ุจุฅุณูุงุท ููุฒุฉ "ุงูุฅุฌุงุจุฉ" ุงูุชู ูุจุฏู ุฃููุง ุชุญุชูู ุนูู ุชูุจุคุงุช ุจูุงุณุทุฉ ูููุฐุฌ ุขุฎุฑ.
ุณูููู ุฃูุถูุง ุจุฃุฎุฐ ุงูุฅุฌุงุจุฉ ุงูุฃููู ูู ูุฌููุนุฉ ุงูุฅุฌุงุจุงุช ุงูุชู ูุฏููุง ุงููุนูููู. ุฃู ููููู ุฃุฎุฐ ุนููุฉ ุนุดูุงุฆูุฉ ูููุง.

```py
>>> updated_dataset = dataset.map(lambda example: {"question": example["query"]["en"]}, remove_columns=["query"])
>>> updated_dataset = updated_dataset.map(
...     lambda example: {"answer": example["answers"][0]}, remove_columns=["answer", "answers"]
... )
```

ูุงุญุธ ุฃู ููุทุฉ ุงูุชุญูู LayoutLMv2 ุงูุชู ูุณุชุฎุฏููุง ูู ูุฐุง ุงูุฏููู ุชู ุชุฏุฑูุจูุง ูุน `max_position_embeddings = 512` (ููููู
ุงูุนุซูุฑ ุนูู ูุฐู ุงููุนูููุงุช ูู ููู `config.json` ุงูุฎุงุต ุจููุทุฉ ุงูุชุญูู [ููุง](https://huggingface.co/microsoft/layoutlmv2-base-uncased/blob/main/config.json#L18)).
ูููููุง ุชูููุต ุงูุฃูุซูุฉ ูููู ูุชุฌูุจ ุงููููู ุงูุฐู ูุฏ ุชููู ููู ุงูุฅุฌุงุจุฉ ูู ููุงูุฉ ูุณุชูุฏ ุทููู ูุชูุชูู ููุชุทุนุฉุ
ููุง ุณูููู ุจุฅุฒุงูุฉ ุงูุฃูุซูุฉ ุงูููููุฉ ุงูุชู ูู ุงููุญุชูู ุฃู ููุชูู ูููุง ุชุถููููุง ุฅูู ุฃูุซุฑ ูู 512.
ุฅุฐุง ูุงูุช ูุนุธู ุงููุณุชูุฏุงุช ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุงูุฎุงุตุฉ ุจู ุทูููุฉุ ูููููู ุชูููุฐ ุฅุณุชุฑุงุชูุฌูุฉ ุงููุงูุฐุฉ ุงูููุฒููุฉ - ุชุญูู ูู [ูุฐุง ุงูุฏูุชุฑ](https://github.com/huggingface/notebooks/blob/main/examples/question_answering.ipynb) ููุญุตูู ุนูู ุงูุชูุงุตูู.

```py
>>> updated_dataset = updated_dataset.filter(lambda x: len(x["words"]) + len(x["question"].split()) < 512)
```

ูู ูุฐู ุงููุฑุญูุฉุ ุฏุนูุง ูุฒูู ุฃูุถูุง ููุฒุงุช ุงูุชุนุฑู ุงูุถูุฆู ุนูู ุงูุญุฑูู ูู ูุฐู ุงููุฌููุนุฉ ูู ุงูุจูุงูุงุช. ูุฐู ูู ูุชูุฌุฉ ุงูุชุนุฑู ุงูุถูุฆู ุนูู ุงูุญุฑูู ูุถุจุท ูููุฐุฌ ูุฎุชูู. ูุง ูุฒุงููู ุจุญุงุฌุฉ ุฅูู ุจุนุถ ุงููุนุงูุฌุฉ ุฅุฐุง ุฃุฑุฏูุง ุงุณุชุฎุฏุงููุงุ ูุฃููุง ูุง ุชุชุทุงุจู ูุน ูุชุทูุจุงุช ุงูุฅุฏุฎุงู
ูู ุงููููุฐุฌ ุงูุฐู ูุณุชุฎุฏูู ูู ูุฐุง ุงูุฏููู. ุจุฏูุงู ูู ุฐููุ ูููููุง ุงุณุชุฎุฏุงู [`LayoutLMv2Processor`] ุนูู ุงูุจูุงูุงุช ุงูุฃุตููุฉ ููู ูู ุงูุชุนุฑู ุงูุถูุฆู ุนูู ุงูุญุฑูู
ูุงูุชูููุฐ. ุจูุฐู ุงูุทุฑููุฉ ุณูุญุตู ุนูู ุงููุฏุฎูุงุช ุงูุชู ุชุชุทุงุจู ูุน ุงูุฅุฏุฎุงู ุงููุชููุน ูููููุฐุฌ. ุฅุฐุง ููุช ุชุฑูุฏ ูุนุงูุฌุฉ ุงูุตูุฑ ูุฏูููุงุ
ุชุญูู ูู ูุซุงุฆู ูููุฐุฌ [`LayoutLMv2`](../model_doc/layoutlmv2) ููุนุฑูุฉ ุชูุณูู ุงูุฅุฏุฎุงู ุงูุฐู ูุชููุนู ุงููููุฐุฌ.

```py
>>> updated_dataset = updated_dataset.remove_columns("words")
>>> updated_dataset = updated_dataset.remove_columns("bounding_boxes")
```

ุฃุฎูุฑูุงุ ูู ูููู ุงุณุชูุดุงู ุงูุจูุงูุงุช ููุชููุงู ุฅุฐุง ูู ูููู ูุธุฑุฉ ุนูู ูุซุงู ุนูู ุงูุตูุฑุฉ.

```py
>>> updated_dataset["train"][11]["image"]
```

<div class="flex justify-center">
     <img src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/docvqa_example.jpg" alt="ูุซุงู DocVQA"/>
 </div>

## ูุนุงูุฌุฉ ุงูุจูุงูุงุช ูุณุจููุง

ูููุฉ ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุงุฑุฏุฉ ูู ุงููุซููุฉ ูู ูููุฉ ูุชุนุฏุฏุฉ ุงููุณุงุฆุทุ ููุฌุจ ุงูุชุฃูุฏ ูู ูุนุงูุฌุฉ ุงููุฏุฎูุงุช ูู ูู ูุณูุท
ููููุง ูุชููุนุงุช ุงููููุฐุฌ. ุฏุนููุง ูุจุฏุฃ ุจุชุญููู [`LayoutLMv2Processor`]ุ ูุงูุฐู ูุฌูุน ุฏุงุฎูููุง ุจูู ูุนุงูุฌ ุงูุตูุฑ ุงูุฐู ููููู ุงูุชุนุงูู ูุน ุจูุงูุงุช ุงูุตูุฑ ููุนุงูุฌ ุงูุฑููุฒ ุงูุฐู ููููู ุชุดููุฑ ุจูุงูุงุช ุงููุต.

```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
```

### ูุนุงูุฌุฉ ุตูุฑ ุงููุณุชูุฏุงุช
```py
>>> from transformers import AutoProcessor

>>> processor = AutoProcessor.from_pretrained(model_checkpoint)
```

### ูุนุงูุฌุฉ ุตูุฑ ุงููุณุชูุฏุงุช

ุฃููุงูุ ุฏุนููุง ูููู ุจุฅุนุฏุงุฏ ุตูุฑ ุงููุณุชูุฏุงุช ูููููุฐุฌ ุจูุณุงุนุฏุฉ `image_processor` ูู ุงููุนุงูุฌ.
ุจุญูู ุงูุชูุตููุ ูููู ูุนุงูุฌ ุงูุตูุฑ ุจุฅุนุงุฏุฉ ุชุญุฌูู ุงูุตูุฑ ุฅูู 224x224ุ ูุงูุชุฃูุฏ ูู ุฃู ูุฏููุง ุงูุชุฑุชูุจ ุงูุตุญูุญ ููููุงุช ุงูุฃููุงูุ
ุชุทุจูู ุงูุชุนุฑู ุงูุถูุฆู ุนูู ุงูุญุฑูู ุจุงุณุชุฎุฏุงู Tesseract ููุญุตูู ุนูู ุงููููุงุช ูุญุฏูุฏ ุงูุฅุญุฏุงุซูุงุช ุงููุนูุงุฑูุฉ. ูู ูุฐุง ุงูุจุฑูุงูุฌ ุงูุชุนููููุ ูุฐู ุงูุงูุชุฑุงุถูุงุช ูู ุจุงูุถุจุท ูุง ูุญุชุงุฌู.
ุงูุชุจ ุฏุงูุฉ ุชุทุจู ุงููุนุงูุฌุฉ ุงูุงูุชุฑุงุถูุฉ ุนูู ุฏูุนุฉ ูู ุงูุตูุฑ ูุชุนูุฏ ูุชุงุฆุฌ ุงูุชุนุฑู ุงูุถูุฆู ุนูู ุงูุญุฑูู.

```py
>>> image_processor = processor.image_processor


>>> def get_ocr_words_and_boxes(examples):
...     images = [image.convert("RGB") for image in examples["image"]]
...     encoded_inputs = image_processor(images)

...     examples["image"] = encoded_inputs.pixel_values
...     examples["words"] = encoded_inputs.words
...     examples["boxes"] = encoded_inputs.boxes

...     return examples
```

ูุชุทุจูู ูุฐู ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุนูู ูุฌููุนุฉ ุงูุจูุงูุงุช ุจุฃููููุง ุจุทุฑููุฉ ุณุฑูุนุฉุ ุงุณุชุฎุฏู [`~datasets.Dataset.map`].

```py
>>> dataset_with_ocr = updated_dataset.map(get_ocr_words_and_boxes, batched=True, batch_size=2)
```

### ูุนุงูุฌุฉ ุจูุงูุงุช ุงููุต

ุจูุฌุฑุฏ ุชุทุจูู ุงูุชุนุฑู ุงูุถูุฆู ุนูู ุงูุตูุฑุ ูุญุชุงุฌ ุฅูู ุชุดููุฑ ุงูุฌุฒุก ุงููุตู ูู ูุฌููุนุฉ ุงูุจูุงูุงุช ูุฅุนุฏุงุฏูุง ูููููุฐุฌ.
ููุทูู ูุฐุง ุนูู ุชุญููู ุงููููุงุช ูุญุฏูุฏ ุงูุฅุญุฏุงุซูุงุช ุงูุชู ุญุตููุง ุนูููุง ูู ุงูุฎุทูุฉ ุงูุณุงุจูุฉ ุฅูู ูุณุชูู ุงูุฑูุฒ `input_ids` ู `attention_mask`
ุ `token_type_ids` ู `bbox`. ููุนุงูุฌุฉ ุงููุตุ ุณูุญุชุงุฌ ุฅูู `tokenizer` ูู ุงููุนุงูุฌ.

```py
>>> tokenizer = processor.tokenizer
```

ุจุงูุฅุถุงูุฉ ุฅูู ุงููุนุงูุฌุฉ ุงููุฐููุฑุฉ ุฃุนูุงูุ ูุญุชุงุฌ ุฃูุถูุง ุฅูู ุฅุถุงูุฉ ุงูุนูุงูุงุช ุฅูู ุงููููุฐุฌ. ุจุงููุณุจุฉ ูููุงุฐุฌ `xxxForQuestionAnswering` ูู ๐ค Transformersุ
ุชุชููู ุงูุนูุงูุงุช ูู `start_positions` ู `end_positions`ุ ูุงูุชู ุชุดูุฑ ุฅูู ุงูุฑูุฒ ุงูููุฌูุฏ ูู
ุจุฏุงูุฉ ูููุงูุฉ ุงูุฅุฌุงุจุฉ.

ุฏุนููุง ูุจุฏุฃ ุจุฐูู. ูู ุจุชุนุฑูู ุฏุงูุฉ ูุณุงุนุฏุฉ ูููููุง ุงูุนุซูุฑ ุนูู ูุงุฆูุฉ ูุฑุนูุฉ (ุงูุฅุฌุงุจุฉ ุงูููุณูุฉ ุฅูู ูููุงุช) ูู ูุงุฆูุฉ ุฃูุจุฑ (ูุงุฆูุฉ ุงููููุงุช).

ุณุชุฃุฎุฐ ูุฐู ุงูุฏุงูุฉ ูุฅุฏุฎุงู ูุงุฆูุชููุ `words_list` ู `answer_list`. ุซู ุณูููู ุจุงูุชููู ุนุจุฑ `words_list` ูุงูุชุญูู
ุฅุฐุง ูุงู ุงููููุฉ ุงูุญุงููุฉ ูู `words_list` (words_list [i]) ูุชุณุงููุฉ ูุน ุงููููุฉ ุงูุฃููู ูู answer_list (answer_list [0]) ูุฅุฐุง
ูุงูุช ุงููุงุฆูุฉ ุงููุฑุนูุฉ ูู `words_list` ุจุฏุกูุง ูู ุงููููุฉ ุงูุญุงููุฉ ูุทูููุง ูุชุณุงูู `ูุน answer_list`.
ุฅุฐุง ูุงู ูุฐุง ุงูุดุฑุท ุตุญูุญูุงุ ููุฐุง ูุนูู ุฃูู ุชู ุงูุนุซูุฑ ุนูู ุชุทุงุจูุ ูุณุชููู ุงูุฏุงูุฉ ุจุชุณุฌูู ุงููุทุงุจูุฉุ ูููุถุน ุงูุจุฏุก ุงูุฎุงุต ุจูุงุ
ูููุถุน ุงูููุงูุฉ (idx + len (answer_list) - 1). ุฅุฐุง ุชู ุงูุนุซูุฑ ุนูู ุฃูุซุฑ ูู ุชุทุงุจู ูุงุญุฏุ ูุณุชุนูุฏ ุงูุฏุงูุฉ ููุท ุงูุฃูู.
ุฅุฐุง ูู ูุชู ุงูุนุซูุฑ ุนูู ุฃู ุชุทุงุจูุ ูุณุชุนูุฏ ุงูุฏุงูุฉ (Noneุ 0ุ ู 0).

```py
>>> def subfinder(words_list, answer_list):
...     matches = []
...     start_indices = []
...     end_indices = []
...     for idx, i in enumerate(range(len(words_list))):
...         if words_list[i] == answer_list[0] and words_list[i : i + len(answer_list)] == answer_list:
...             matches.append(answer_list)
...             start_indices.append(idx)
...             end_indices.append(idx + len(answer_list) - 1)
...     if matches:
...         return matches[0], start_indices[0], end_indices[0]
...     else:
...         return None, 0, 0
```

ูุชูุถูุญ ููููุฉ ุนุซูุฑ ูุฐู ุงูุฏุงูุฉ ุนูู ููุถุน ุงูุฅุฌุงุจุฉุ ุฏุนูุง ูุณุชุฎุฏููุง ูู ูุซุงู:
ูุชูุถูุญ ููููุฉ ุนุซูุฑ ูุฐู ุงูุฏุงูุฉ ุนูู ููุถุน ุงูุฅุฌุงุจุฉุ ุฏุนูุง ูุณุชุฎุฏููุง ูู ูุซุงู:

```py
>>> example = dataset_with_ocr["train"][1]
>>> words = [word.lower() for word in example["words"]]
>>> match, word_idx_start, word_idx_end = subfinder(words, example["answer"].lower().split())
>>> print("Question: ", example["question"])
>>> print("Words:", words)
>>> print("Answer: ", example["answer"])
>>> print("start_index", word_idx_start)
>>> print("end_index", word_idx_end)
Question:  ูู ูู ูู ุณู ุณู ูู ูุฐู ุงูุฑุณุงูุฉุ
Words: ['wie', 'baw', 'brown', '&', 'williamson', 'tobacco', 'corporation', 'research', '&', 'development', 'internal', 'correspondence', 'to:', 'r.', 'h.', 'honeycutt', 'ce:', 't.f.', 'riehl', 'from:', '.', 'c.j.', 'cook', 'date:', 'may', '8,', '1995', 'subject:', 'review', 'of', 'existing', 'brainstorming', 'ideas/483', 'the', 'major', 'function', 'of', 'the', 'product', 'innovation', 'graup', 'is', 'to', 'develop', 'marketable', 'nove!', 'products', 'that', 'would', 'be', 'profitable', 'to', 'manufacture', 'and', 'sell.', 'novel', 'is', 'defined', 'as:', 'of', 'a', 'new', 'kind,', 'or', 'different', 'from', 'anything', 'seen', 'or', 'known', 'before.', 'innovation', 'is', 'defined', 'as:', 'something', 'new', 'or', 'different', 'introduced;', 'act', 'of', 'innovating;', 'introduction', 'of', 'new', 'things', 'or', 'methods.', 'the', 'products', 'may', 'incorporate', 'the', 'latest', 'technologies,', 'materials', 'and', 'know-how', 'available', 'to', 'give', 'then', 'a', 'unique', 'taste', 'or', 'look.', 'the', 'first', 'task', 'of', 'the', 'product', 'innovation', 'group', 'was', 'to', 'assemble,', 'review', 'and', 'categorize', 'a', 'list', 'of', 'existing', 'brainstorming', 'ideas.', 'ideas', 'were', 'grouped', 'into', 'two', 'major', 'categories', 'labeled', 'appearance', 'and', 'taste/aroma.', 'these', 'categories', 'are', 'used', 'for', 'novel', 'products', 'that', 'may', 'differ', 'from', 'a', 'visual', 'and/or', 'taste/aroma', 'point', 'of', 'view', 'compared', 'to', 'canventional', 'cigarettes.', 'other', 'categories', 'include', 'a', 'combination', 'of', 'the', 'above,', 'filters,', 'packaging', 'and', 'brand', 'extensions.', 'appearance', 'this', 'category', 'is', 'used', 'for', 'novel', 'cigarette', 'constructions', 'that', 'yield', 'visually', 'different', 'products', 'with', 'minimal', 'changes', 'in', 'smoke', 'chemistry', 'two', 'cigarettes', 'in', 'cne.', 'emulti-plug', 'te', 'build', 'yaur', 'awn', 'cigarette.', 'eswitchable', 'menthol', 'or', 'non', 'menthol', 'cigarette.', '*cigarettes', 'with', 'interspaced', 'perforations', 'to', 'enable', 'smoker', 'to', 'separate', 'unburned', 'section', 'for', 'future', 'smoking.', 'ยซshort', 'cigarette,', 'tobacco', 'section', '30', 'mm.', 'ยซextremely', 'fast',
ูุฐุง ูู ุงููุต ุงููุชุฑุฌู ูุน ุงุชุจุงุน ุงูุชุนูููุงุช ุงูุชู ูุฏูุชูุง: 

ุฏุนููุง ูุชุญูู ูู ุดูู ุฎุตุงุฆุต ูุฌููุนุฉ ุงูุจูุงูุงุช ุงููุดูุฑุฉ:

```py
>>> encoded_train_dataset.features
{'image': Sequence(feature=Sequence(feature=Sequence(feature=Value(dtype='uint8', id=None), length=-1, id=None), length=-1, id=None), length=-1, id=None),
 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None),
 'token_type_ids': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),
 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None),
 'bbox': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None),
 'start_positions': Value(dtype='int64', id=None),
 'end_positions': Value(dtype='int64', id=None)}
```

## ุงูุชูููู

ูุชุทูุจ ุชูููู ุงูุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุชุนููุฉ ุจุงููุซุงุฆู ูุฏุฑูุง ูุจูุฑูุง ูู ุงููุนุงูุฌุฉ ุงููุงุญูุฉ. ููุชุฌูุจ ุงุณุชุบุฑุงู ุงููุซูุฑ ูู ููุชูุ ูุชุฎุทู ูุฐุง ุงูุฏููู ุฎุทูุฉ ุงูุชูููู. ูุง ูุฒุงู [`Trainer`] ูุญุณุจ ุฎุณุงุฑุฉ ุงูุชูููู ุฃุซูุงุก ุงูุชุฏุฑูุจ ุญุชู ุชุธู ุนูู ุฏุฑุงูุฉ ุจุฃุฏุงุก ูููุฐุฌู. ุนุงุฏุฉู ูุง ูุชู ุชูููู ุงูุฅุฌุงุจุฉ ุงูุงุณุชุฎุฑุงุฌูุฉ ุนูู ุงูุฃุณุฆูุฉ ุจุงุณุชุฎุฏุงู F1/exact match.

ุฅุฐุง ููุช ุชุฑุบุจ ูู ุชูููุฐูุง ุจููุณูุ ูุฑุงุฌุน ูุตู [Question Answering](https://huggingface.co/course/chapter7/7?fw=pt#postprocessing) ูู ุฏูุฑุฉ Hugging Face ููุงุณุชููุงู.

## ุงูุชุฏุฑูุจ

ุชูุงูููุง! ููุฏ ูุฌุญุช ูู ุชุฎุทู ุฃุตุนุจ ุฌุฒุก ูู ูุฐุง ุงูุฏูููุ ูุงูุขู ุฃูุช ูุณุชุนุฏ ูุชุฏุฑูุจ ูููุฐุฌู ุงูุฎุงุต. ููุทูู ุงูุชุฏุฑูุจ ุนูู ุงูุฎุทูุงุช ุงูุชุงููุฉ:

* ูู ุจุชุญููู ุงููููุฐุฌ ุจุงุณุชุฎุฏุงู [`AutoModelForDocumentQuestionAnswering`] ุจุงุณุชุฎุฏุงู ููุณ ููุทุฉ ุงูุชูุชูุด ููุง ูู ูุฑุญูุฉ ูุง ูุจู ุงููุนุงูุฌุฉ.
* ุญุฏุฏ ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ ุงูุฎุงุตุฉ ุจู ูู [`TrainingArguments`].
* ุญุฏุฏ ุฏุงูุฉ ูุฏูุฌ ุงูุฃูุซูุฉ ูุนูุงุ ุญูุซ ุณุชููู [`DefaultDataCollator`] ููุงุณุจุฉ ุชูุงููุง
* ูู ุจุชูุฑูุฑ ูุฑุท ูุนููุงุช ุงูุชุฏุฑูุจ ุฅูู [`Trainer`] ุฌูุจูุง ุฅูู ุฌูุจ ูุน ุงููููุฐุฌ ููุฌููุนุฉ ุงูุจูุงูุงุช ูุฏูุฌ ุงูุจูุงูุงุช.
* ุงุณุชุฏุนุงุก [`~Trainer.train`] ูุถุจุท ูููุฐุฌู ุงูุฏููู.

```py
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained(model_checkpoint)
```

ูู [`TrainingArguments`]ุ ุงุณุชุฎุฏู `output_dir` ูุชุญุฏูุฏ ููุงู ุญูุธ ูููุฐุฌูุ ููู ุจุชูููู ูุฑุท ุงููุนููุงุช ููุง ุชุฑุงู ููุงุณุจูุง.

ุฅุฐุง ููุช ุชุฑุบุจ ูู ูุดุงุฑูุฉ ูููุฐุฌู ูุน ุงููุฌุชูุนุ ูู ุจุชุนููู `push_to_hub` ุฅูู `True` (ูุฌุจ ุฃู ุชููู ูุฏ ุณุฌูุช ุงูุฏุฎูู ุฅูู Hugging Face ูุชุญููู ูููุฐุฌู). ูู ูุฐู ุงูุญุงูุฉุ ุณูููู `output_dir` ุฃูุถูุง ุงุณู ุงููุณุชูุฏุน ุญูุซ ุณูุชู ุฏูุน ููุทุฉ ุชูุชูุด ุงููููุฐุฌ ุงูุฎุงุต ุจู.

```py
>>> from transformers import TrainingArguments

>>> # ุงุณุชุจุฏู ูุฐุง ุจูุนุฑู ูุณุชูุฏุนู
>>> repo_id = "MariaK/layoutlmv2-base-uncased_finetuned_docvqa"

>>> training_args = TrainingArguments(
...     output_dir=repo_id,
...     per_device_train_batch_size=4,
...     num_train_epochs=20,
...     save_steps=200,
...     logging_steps=50,
...     eval_strategy="steps",
...     learning_rate=5e-5,
...     save_total_limit=2,
...     remove_unused_columns=False,
...     push_to_hub=True,
... )
```

ูู ุจุชุนุฑูู ุฏุงูุฉ ุจุณูุทุฉ ูุฏูุฌ ุงูุจูุงูุงุช ูุฏูุฌ ุงูุฃูุซูุฉ ูุนูุง.

```py
>>> from transformers import DefaultDataCollator

>>> data_collator = DefaultDataCollator()
```

ุฃุฎูุฑูุงุ ูู ุจุฌูุน ูู ุดูุก ูุนูุงุ ูุงุณุชุฏุนุงุก [`~Trainer.train`]:

```py
>>> from transformers import Trainer

>>> trainer = Trainer(
...     model=model,
...     args=training_args,
...     data_collator=data_collator,
...     train_dataset=encoded_train_dataset,
...     eval_dataset=encoded_test_dataset,
...     tokenizer=processor,
... )

>>> trainer.train()
```

ูุฅุถุงูุฉ ุงููููุฐุฌ ุงูููุงุฆู ุฅูู ๐ค Hubุ ูู ุจุฅูุดุงุก ุจุทุงูุฉ ูููุฐุฌ ูุงุณุชุฏุนุงุก `push_to_hub`:

```py
>>> trainer.create_model_card()
>>> trainer.push_to_hub()
```

## ุงูุงุณุชูุชุงุฌ

ุงูุขู ุจุนุฏ ุฃู ููุช ุจุถุจุท ูููุฐุฌ LayoutLMv2 ูุชุญูููู ุฅูู ๐ค Hubุ ููููู ุงุณุชุฎุฏุงูู ููุงุณุชูุชุงุฌ. ุฃุณูู ุทุฑููุฉ ูุชุฌุฑุจุฉ ูููุฐุฌู ุงููุถุจูุท ููุงุณุชูุชุงุฌ ูู ุงุณุชุฎุฏุงูู ูู [`Pipeline`].

ุฏุนูุง ูุฃุฎุฐ ูุซุงูุงู:

```py
>>> example = dataset["test"][2]
>>> question = example["query"]["en"]
>>> image = example["image"]
>>> print(question)
>>> print(example["answers"])
'Who is โpresidingโ TRRF GENERAL SESSION (PART 1)?'
['TRRF Vice President', 'lee a. waller']
```

ุจุนุฏ ุฐููุ ูู ุจุชูููุฐ ุฎุท ุฃูุงุจูุจ ููุฅุฌุงุจุฉ ุนูู ุงูุฃุณุฆูุฉ ุงููุชุนููุฉ ุจุงููุซุงุฆู ุจุงุณุชุฎุฏุงู ูููุฐุฌูุ ููุฑุฑ ูุฒูุฌ ุงูุตูุฑุฉ + ุงูุณุคุงู ุฅููู.

```py
>>> from transformers import pipeline

>>> qa_pipeline = pipeline("document-question-answering", model="MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> qa_pipeline(image, question)
[{'score': 0.9949808120727539,
  'answer': 'Lee A. Waller',
  'start': 55,
  'end': 57}]
```

ููููู ุฃูุถูุง ูุญุงูุงุฉ ูุชุงุฆุฌ ุฎุท ุงูุฃูุงุจูุจ ูุฏูููุง ุฅุฐุง ููุช ุชุฑุบุจ ูู ุฐูู:

1. ุฎุฐ ุตูุฑุฉ ูุณุคุงูุ ููู ุจุฅุนุฏุงุฏููุง ูููููุฐุฌ ุจุงุณุชุฎุฏุงู ุงููุนุงูุฌ ูู ูููุฐุฌู.
2. ูู ุจุชูุฑูุฑ ูุชูุฌุฉ ุงููุนุงูุฌุฉ ุงููุณุจูุฉ ุนุจุฑ ุงููููุฐุฌ.
3. ูุนูุฏ ุงููููุฐุฌ `start_logits` ู`end_logits`ุ ูุงูุชู ุชุดูุฑ ุฅูู ุงูุฑูุฒ ุงูุฐู ูููู ูู ุจุฏุงูุฉ ุงูุฅุฌุงุจุฉ ูุงูุฑูุฒ ุงูุฐู ูููู ูู ููุงูุฉ ุงูุฅุฌุงุจุฉ. ููุงููุง ูู ุดูู (batch_sizeุ sequence_length).
4. ูู ุจุฅุฌุฑุงุก argmax ุนูู ุงูุจุนุฏ ุงูุฃุฎูุฑ ููู ูู `start_logits` ู`end_logits` ููุญุตูู ุนูู `start_idx` ุงููุชููุน ู`end_idx`.
5. ูู ุชุดููุฑ ุงูุฅุฌุงุจุฉ ุจุงุณุชุฎุฏุงู ุงููุนุงูุฌ.

```py
>>> import torch
>>> from transformers import AutoProcessor
>>> from transformers import AutoModelForDocumentQuestionAnswering

>>> processor = AutoProcessor.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")
>>> model = AutoModelForDocumentQuestionAnswering.from_pretrained("MariaK/layoutlmv2-base-uncased_finetuned_docvqa")

>>> with torch.no_grad():
...     encoding = processor(image.convert("RGB"), question, return_tensors="pt")
...     outputs = model(**encoding)
...     start_logits = outputs.start_logits
...     end_logits = outputs.end_logits
...     predicted_start_idx = start_logits.argmax(-1).item()
...     predicted_end_idx = end_logits.argmax(-1).item()

>>> processor.tokenizer.decode(encoding.input_ids.squeeze()[predicted_start_idx : predicted_end_idx + 1])
'lee a. waller'
```